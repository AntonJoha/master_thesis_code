{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecc6862-1f4c-4dbc-878a-28c97a990926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from parse_data import get_data, get_modified_values, get_binary_values, make_data_scalar\n",
    "from rnn_cell import PredictTime\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a932cf-5ecf-4c1f-9513-2400c8cb8686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]], device='cuda:0'),\n",
       " tensor([[1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.]], device='cuda:0'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_data(df, device):\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    prev = None\n",
    "\n",
    "    for row in df:\n",
    "        if prev is None:\n",
    "            prev = row\n",
    "        x_train.append(prev)\n",
    "        y_train.append(row)\n",
    "        prev = row\n",
    "    print(x_train[0].size())\n",
    "    return torch.stack(x_train).float().to(device),torch.stack(y_train).float().to(device)\n",
    "\n",
    "make_data(get_binary_values(get_data()), device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f71633-04f1-488e-8421-3df5644a11a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  1000\n",
      "torch.Size([8152, 1]) torch.Size([8152, 1])\n",
      "Epoch 0 Loss 16.5944\n",
      "Epoch 10 Loss 14.8649\n",
      "Epoch 20 Loss 14.8731\n",
      "Epoch 30 Loss 14.7688\n",
      "Epoch 40 Loss 14.7558\n",
      "Epoch 50 Loss 14.7164\n",
      "Epoch 60 Loss 14.6901\n",
      "Epoch 70 Loss 14.5990\n",
      "Epoch 80 Loss 14.5690\n",
      "Epoch 90 Loss 14.5044\n",
      "Epoch 100 Loss 14.4620\n",
      "Epoch 110 Loss 14.4385\n",
      "Epoch 120 Loss 14.4359\n",
      "Epoch 130 Loss 14.3682\n",
      "Epoch 140 Loss 14.3404\n",
      "Epoch 150 Loss 14.3133\n",
      "Epoch 160 Loss 14.3321\n",
      "Epoch 170 Loss 14.2625\n",
      "Epoch 180 Loss 14.2787\n",
      "Epoch 190 Loss 14.2563\n",
      "Epoch 200 Loss 14.2144\n",
      "Epoch 210 Loss 14.2077\n",
      "Epoch 220 Loss 14.1991\n",
      "Epoch 230 Loss 14.2095\n",
      "Epoch 240 Loss 14.1927\n",
      "Epoch 250 Loss 14.1858\n",
      "Epoch 260 Loss 14.1869\n",
      "Epoch 270 Loss 14.1820\n",
      "Epoch 280 Loss 14.1846\n",
      "Epoch 290 Loss 14.1798\n",
      "Epoch 300 Loss 14.1720\n",
      "Epoch 310 Loss 14.9659\n",
      "Epoch 320 Loss 14.4288\n",
      "Epoch 330 Loss 14.2390\n",
      "Epoch 340 Loss 14.2212\n",
      "Epoch 350 Loss 14.1966\n",
      "Epoch 360 Loss 14.1961\n",
      "Epoch 370 Loss 14.1824\n",
      "Epoch 380 Loss 14.4109\n",
      "Epoch 390 Loss 14.2812\n",
      "Epoch 400 Loss 14.8325\n",
      "Epoch 410 Loss 14.2200\n",
      "Epoch 420 Loss 14.2027\n",
      "Epoch 430 Loss 14.1907\n",
      "Epoch 440 Loss 14.2858\n",
      "Epoch 450 Loss 14.1853\n",
      "Epoch 460 Loss 14.1701\n",
      "Epoch 470 Loss 14.1610\n",
      "Epoch 480 Loss 14.1543\n",
      "Epoch 490 Loss 14.1497\n",
      "Epoch 500 Loss 14.1451\n",
      "Epoch 510 Loss 14.1415\n",
      "Epoch 520 Loss 14.1349\n",
      "Epoch 530 Loss 14.1289\n",
      "Epoch 540 Loss 14.1246\n",
      "Epoch 550 Loss 14.1245\n",
      "Epoch 560 Loss 14.1079\n",
      "Epoch 570 Loss 14.1223\n",
      "Epoch 580 Loss 14.0957\n",
      "Epoch 590 Loss 14.0816\n",
      "Epoch 600 Loss 14.1335\n",
      "Epoch 610 Loss 14.0719\n",
      "Epoch 620 Loss 14.0708\n",
      "Epoch 630 Loss 14.1258\n",
      "Epoch 640 Loss 14.0661\n",
      "Epoch 650 Loss 14.0498\n",
      "Epoch 660 Loss 14.0283\n",
      "Epoch 670 Loss 14.0154\n",
      "Epoch 680 Loss 13.9947\n",
      "Epoch 690 Loss 14.3424\n",
      "Epoch 700 Loss 14.1452\n",
      "Epoch 710 Loss 14.0807\n",
      "Epoch 720 Loss 14.8389\n",
      "Epoch 730 Loss 14.0162\n",
      "Epoch 740 Loss 14.0519\n",
      "Epoch 750 Loss 14.0715\n",
      "Epoch 760 Loss 13.9871\n",
      "Epoch 770 Loss 13.9449\n",
      "Epoch 780 Loss 14.8316\n",
      "Epoch 790 Loss 13.8744\n",
      "Epoch 800 Loss 13.8832\n",
      "Epoch 810 Loss 13.8162\n",
      "Epoch 820 Loss 14.8328\n",
      "Epoch 830 Loss 13.8341\n",
      "Epoch 840 Loss 13.7623\n",
      "Epoch 850 Loss 13.7200\n",
      "Epoch 860 Loss 13.7602\n",
      "Epoch 870 Loss 13.7171\n",
      "Epoch 880 Loss 13.6298\n",
      "Epoch 890 Loss 13.7516\n",
      "Epoch 900 Loss 13.7448\n",
      "Epoch 910 Loss 14.8250\n",
      "Epoch 920 Loss 13.7393\n",
      "Epoch 930 Loss 13.6359\n",
      "Epoch 940 Loss 13.5341\n",
      "Epoch 950 Loss 13.5104\n",
      "Epoch 960 Loss 13.6079\n",
      "Epoch 970 Loss 13.6533\n",
      "Epoch 980 Loss 13.4550\n",
      "Epoch 990 Loss 13.3964\n",
      "Epoch 1000 Loss 13.4113\n",
      "Epoch 1010 Loss 13.4028\n",
      "Epoch 1020 Loss 14.8334\n",
      "Epoch 1030 Loss 13.2541\n",
      "Epoch 1040 Loss 13.4411\n",
      "Epoch 1050 Loss 13.3963\n",
      "Epoch 1060 Loss 13.1804\n",
      "Epoch 1070 Loss 13.1648\n",
      "Epoch 1080 Loss 13.1009\n",
      "Epoch 1090 Loss 13.0889\n",
      "Epoch 1100 Loss 13.0558\n",
      "Epoch 1110 Loss 12.9397\n",
      "Epoch 1120 Loss 12.7917\n",
      "Epoch 1130 Loss 12.8059\n",
      "Epoch 1140 Loss 14.9191\n",
      "Epoch 1150 Loss 13.0224\n",
      "Epoch 1160 Loss 12.8327\n",
      "Epoch 1170 Loss 12.9698\n",
      "Epoch 1180 Loss 12.8633\n",
      "Epoch 1190 Loss 12.5923\n",
      "Epoch 1200 Loss 12.5682\n",
      "Epoch 1210 Loss 12.5437\n",
      "Epoch 1220 Loss 15.2099\n",
      "Epoch 1230 Loss 14.3319\n",
      "Epoch 1240 Loss 14.1177\n",
      "Epoch 1250 Loss 14.0332\n",
      "Epoch 1260 Loss 13.9779\n",
      "Epoch 1270 Loss 13.9796\n",
      "Epoch 1280 Loss 13.9079\n",
      "Epoch 1290 Loss 14.0838\n",
      "Epoch 1300 Loss 14.1740\n",
      "Epoch 1310 Loss 13.9333\n",
      "Epoch 1320 Loss 13.8651\n",
      "Epoch 1330 Loss 13.7931\n",
      "Epoch 1340 Loss 13.7361\n",
      "Epoch 1350 Loss 13.7059\n",
      "Epoch 1360 Loss 13.6543\n",
      "Epoch 1370 Loss 13.6212\n",
      "Epoch 1380 Loss 13.5693\n",
      "Epoch 1390 Loss 14.8376\n",
      "Epoch 1400 Loss 13.5369\n",
      "Epoch 1410 Loss 13.4732\n",
      "Epoch 1420 Loss 13.4274\n",
      "Epoch 1430 Loss 13.3807\n",
      "Epoch 1440 Loss 13.3590\n",
      "Epoch 1450 Loss 13.3821\n",
      "Epoch 1460 Loss 13.3048\n",
      "Epoch 1470 Loss 14.8376\n",
      "Epoch 1480 Loss 13.1683\n",
      "Epoch 1490 Loss 13.1723\n",
      "Epoch 1500 Loss 13.3506\n",
      "Epoch 1510 Loss 13.2317\n",
      "Epoch 1520 Loss 14.8533\n",
      "Epoch 1530 Loss 13.1316\n",
      "Epoch 1540 Loss 13.0204\n",
      "Epoch 1550 Loss 12.9265\n",
      "Epoch 1560 Loss 14.8488\n",
      "Epoch 1570 Loss 13.2476\n",
      "Epoch 1580 Loss 13.1699\n",
      "Epoch 1590 Loss 13.5919\n",
      "Epoch 1600 Loss 13.4173\n",
      "Epoch 1610 Loss 12.9709\n",
      "Epoch 1620 Loss 12.8946\n",
      "Epoch 1630 Loss 12.7648\n",
      "Epoch 1640 Loss 14.8463\n",
      "Epoch 1650 Loss 12.5490\n",
      "Epoch 1660 Loss 12.4547\n",
      "Epoch 1670 Loss 14.8557\n",
      "Epoch 1680 Loss 12.4495\n",
      "Epoch 1690 Loss 12.2979\n",
      "Epoch 1700 Loss 12.3582\n",
      "Epoch 1710 Loss 12.2467\n",
      "Epoch 1720 Loss 12.7816\n",
      "Epoch 1730 Loss 15.0501\n",
      "Epoch 1740 Loss 12.2791\n",
      "Epoch 1750 Loss 12.2416\n",
      "Epoch 1760 Loss 13.2443\n",
      "Epoch 1770 Loss 13.1761\n",
      "Epoch 1780 Loss 13.1210\n",
      "Epoch 1790 Loss 15.2116\n",
      "Epoch 1800 Loss 14.6157\n",
      "Epoch 1810 Loss 14.1788\n",
      "Epoch 1820 Loss 14.1191\n",
      "Epoch 1830 Loss 14.0757\n",
      "Epoch 1840 Loss 13.9568\n",
      "Epoch 1850 Loss 13.8966\n",
      "Epoch 1860 Loss 13.8610\n",
      "Epoch 1870 Loss 13.8139\n",
      "Epoch 1880 Loss 14.8370\n",
      "Epoch 1890 Loss 13.6812\n",
      "Epoch 1900 Loss 13.6687\n",
      "Epoch 1910 Loss 13.6003\n",
      "Epoch 1920 Loss 13.5325\n",
      "Epoch 1930 Loss 13.5004\n",
      "Epoch 1940 Loss 13.4811\n",
      "Epoch 1950 Loss 13.4150\n",
      "Epoch 1960 Loss 13.3622\n",
      "Epoch 1970 Loss 14.8451\n",
      "Epoch 1980 Loss 13.4802\n",
      "Epoch 1990 Loss 13.3652\n",
      "Epoch 2000 Loss 14.8328\n",
      "Epoch 2010 Loss 13.3013\n",
      "Epoch 2020 Loss 13.1799\n",
      "Epoch 2030 Loss 13.2260\n",
      "Epoch 2040 Loss 13.0595\n",
      "Epoch 2050 Loss 13.0823\n",
      "Epoch 2060 Loss 14.8391\n",
      "Epoch 2070 Loss 14.8445\n",
      "Epoch 2080 Loss 12.8599\n",
      "Epoch 2090 Loss 12.8645\n",
      "Epoch 2100 Loss 12.8316\n",
      "Epoch 2110 Loss 12.7884\n",
      "Epoch 2120 Loss 12.8837\n",
      "Epoch 2130 Loss 14.8428\n",
      "Epoch 2140 Loss 12.7678\n",
      "Epoch 2150 Loss 13.1973\n",
      "Epoch 2160 Loss 12.8627\n",
      "Epoch 2170 Loss 12.7421\n",
      "Epoch 2180 Loss 14.8640\n",
      "Epoch 2190 Loss 12.8528\n",
      "Epoch 2200 Loss 12.5512\n",
      "Epoch 2210 Loss 12.4670\n",
      "Epoch 2220 Loss 12.4075\n",
      "Epoch 2230 Loss 12.3554\n",
      "Epoch 2240 Loss 12.3429\n",
      "Epoch 2250 Loss 12.4174\n",
      "Epoch 2260 Loss 12.3882\n",
      "Epoch 2270 Loss 12.3154\n",
      "Epoch 2280 Loss 12.2470\n",
      "Epoch 2290 Loss 12.2482\n",
      "Epoch 2300 Loss 12.1202\n",
      "Epoch 2310 Loss 12.0484\n",
      "Epoch 2320 Loss 11.9859\n",
      "Epoch 2330 Loss 14.8546\n",
      "Epoch 2340 Loss 11.8970\n",
      "Epoch 2350 Loss 14.8559\n",
      "Epoch 2360 Loss 12.2512\n",
      "Epoch 2370 Loss 12.4746\n",
      "Epoch 2380 Loss 12.2931\n",
      "Epoch 2390 Loss 11.8979\n",
      "Epoch 2400 Loss 14.8539\n",
      "Epoch 2410 Loss 14.8510\n",
      "Epoch 2420 Loss 11.8804\n",
      "Epoch 2430 Loss 14.8662\n",
      "Epoch 2440 Loss 11.7155\n",
      "Epoch 2450 Loss 11.6264\n",
      "Epoch 2460 Loss 11.5574\n",
      "Epoch 2470 Loss 11.4348\n",
      "Epoch 2480 Loss 11.4467\n",
      "Epoch 2490 Loss 14.8617\n",
      "Epoch 2500 Loss 11.3359\n",
      "Epoch 2510 Loss 14.8600\n",
      "Epoch 2520 Loss 11.3317\n",
      "Epoch 2530 Loss 14.8521\n",
      "Epoch 2540 Loss 14.8425\n",
      "Epoch 2550 Loss 12.0404\n",
      "Epoch 2560 Loss 12.2577\n",
      "Epoch 2570 Loss 13.3646\n",
      "Epoch 2580 Loss 15.1265\n",
      "Epoch 2590 Loss 12.6903\n",
      "Epoch 2600 Loss 12.2595\n",
      "Epoch 2610 Loss 11.9873\n",
      "Epoch 2620 Loss 11.7588\n",
      "Epoch 2630 Loss 11.5999\n",
      "Epoch 2640 Loss 11.4984\n",
      "Epoch 2650 Loss 11.5828\n",
      "Epoch 2660 Loss 11.4367\n",
      "Epoch 2670 Loss 11.3422\n",
      "Epoch 2680 Loss 11.1754\n",
      "Epoch 2690 Loss 11.1295\n",
      "Epoch 2700 Loss 11.3915\n",
      "Epoch 2710 Loss 11.6646\n",
      "Epoch 2720 Loss 14.8417\n",
      "Epoch 2730 Loss 11.3339\n",
      "Epoch 2740 Loss 11.0310\n",
      "Epoch 2750 Loss 14.8613\n",
      "Epoch 2760 Loss 10.7602\n",
      "Epoch 2770 Loss 10.6945\n",
      "Epoch 2780 Loss 10.6566\n",
      "Epoch 2790 Loss 10.6366\n",
      "Epoch 2800 Loss 14.8827\n",
      "Epoch 2810 Loss 11.2114\n",
      "Epoch 2820 Loss 14.8822\n",
      "Epoch 2830 Loss 10.5767\n",
      "Epoch 2840 Loss 10.6155\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "batch_size = 200\n",
    "x_d, y_d = make_data_scalar(get_data(), device)\n",
    "\n",
    "print(y_d.size(), x_d.size())\n",
    "\n",
    "model = PredictTime(input_size=x_d[0].size()[0],\n",
    "                    output_size=y_d[0].size()[0],\n",
    "                    hidden_layers=5,\n",
    "                    hidden_size=100, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(x_d,y_d), batch_size=batch_size)\n",
    "epochs = 5000\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    #print(next(iter(loader)))\n",
    "    model.clean_state()\n",
    "    res = []\n",
    "    \n",
    "    \n",
    "    if random.random() < e/(2*epochs):\n",
    "        model.teacher_forcing = False\n",
    "    else:\n",
    "        model.teacher_forcing = True\n",
    "    \n",
    "    for x, y in loader:\n",
    "        \n",
    "        if random.random() < 0.3:\n",
    "            continue\n",
    "        model.init_state()\n",
    "        \n",
    "        \n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y)\n",
    "        res.append(l)\n",
    "        #print(y_pred, y)\n",
    "    \n",
    "    \n",
    "    l = res[0]\n",
    "    for i in res[1:]:\n",
    "        l += i\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    if e % 10 != 0:\n",
    "        continue\n",
    "    #clear_output(wait=True)\n",
    "    sum_loss = 0\n",
    "    #print(list(model.parameters())[-1])\n",
    "\n",
    "    for x, y in loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            sum_loss += np.sqrt(loss(y_pred, y).cpu())\n",
    "    \n",
    "    print(\"Epoch %d Loss %.4f\" % (e, sum_loss))\n",
    "    \n",
    "    \n",
    "#for d in df.values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e66f0-9596-4127-b7c4-835709e375e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_model(x,y,m):\n",
    "    df = get_data() \n",
    "    maxtime = df.max()[1]\n",
    "\n",
    "    res = []\n",
    "    m.eval()\n",
    "    m.clean_state()\n",
    "    count  = 1\n",
    "    prev = x[0][0]\n",
    "    for i in x:\n",
    "        t = torch.tensor([[prev]]).to(device)\n",
    "        prev = i\n",
    "        val = m(t)\n",
    "        count += 1\n",
    "        res.append(val.detach().cpu()[0])\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    print(np.sqrt(loss(torch.tensor(res).to(device),y.squeeze()).cpu()))\n",
    "    \n",
    "    fig, ax = plt.subplots(2)\n",
    "    \n",
    "    \n",
    "    ax[0].plot(range(1,51), res[:50])\n",
    "    ax[0].plot( range(1,51), y[:50].cpu())\n",
    "\n",
    "    ax[1].plot(range(2000,2050), res[2000:2050])\n",
    "    \n",
    "    ax[1].plot(range(2000,2050), y[2000:2050].cpu() )\n",
    "    fig.suptitle(\"Result when feeding correct values as input, mixed teacher forcing and not forcing\")\n",
    "    fig.savefig(\"mixed_teacher_forcing.png\")\n",
    "    \n",
    "model.teacher_forcing = True\n",
    "\n",
    "amount = 1000\n",
    "eval_model(x_d,y_d,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa2663-d7c6-4407-8f5d-e8e3ab650e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_model(x,y,m):\n",
    "    df = get_data() \n",
    "    maxtime = df.max()[1]\n",
    "\n",
    "    res = []\n",
    "    m.eval()\n",
    "    m.clean_state()\n",
    "    count  = 1\n",
    "    prev = x[0][0]\n",
    "    for i in x:\n",
    "        t = torch.tensor([[prev]]).to(device)\n",
    "        prev = i\n",
    "        val = m(t)\n",
    "        prev = val\n",
    "        count += 1\n",
    "        res.append(val.detach().cpu()[0])\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    print(np.sqrt(loss(torch.tensor(res).to(device),y.squeeze()).cpu()))\n",
    "    \n",
    "    fig, ax = plt.subplots(2)\n",
    "    \n",
    "    \n",
    "    ax[0].plot(range(1,51), res[:50])\n",
    "    ax[0].plot( range(1,51), y[:50].cpu())\n",
    "\n",
    "    ax[1].plot(range(2000,2050), res[2000:2050])\n",
    "    \n",
    "    ax[1].plot(range(2000,2050), y[2000:2050].cpu() )\n",
    "    fig.suptitle(\"Result without feeding correct values as input, mixed teacher forcing and not forcing\")\n",
    "    fig.savefig(\"mixed_teacher_forcing_not_correct.png\")\n",
    "\n",
    "model.teacher_forcing = True\n",
    "amount = 1000\n",
    "eval_model(x_d,y_d,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186992f6-b7a6-446e-9556-d36b32bb5bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
