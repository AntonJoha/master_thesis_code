{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ae4109-46bb-4a1f-9dfe-ee57e7219d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93331909-b015-4f44-b0bd-d00b7ff9302f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from parse_data import get_data, get_modified_values, get_binary_values, make_data_scalar\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "\n",
    "\n",
    "def make_data(df, device, seq_len):\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    prev = []\n",
    "    m = df.max()[0]\n",
    "    #print(df)\n",
    "    for row in df.values:\n",
    "        \n",
    "        if len(prev) < seq_len:\n",
    "            before = [0]*(seq_len - len(prev))\n",
    "            for a in prev:\n",
    "                before.append(a)\n",
    "            #print(before)\n",
    "            x_train.append(before)\n",
    "        else:   \n",
    "            x_train.append(prev[-seq_len:])\n",
    "        y_train.append(row[0]/m)\n",
    "        prev.append(row[0]/m)\n",
    "    return x_train,y_train\n",
    "\n",
    "X, y = make_data(get_data(), None, 100)\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f902a51-c4a6-457f-8343-c0db93646091",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.075499 Actual:  0.06655574043261231\n",
      "1 7952\n"
     ]
    }
   ],
   "source": [
    "# gradient boosting for classification in scikit-learn\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "#model = GradientBoostingRegression()\n",
    "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "#n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "#print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "X = X[200:]\n",
    "y = y[200:]\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X, y)\n",
    "# make a single prediction\n",
    "row = X[200:]\n",
    "#print(row)\n",
    "yhat = model.predict(X)\n",
    "print('Prediction: %f' % yhat[65], \"Actual: \", y[65])\n",
    "\n",
    "correct = 0\n",
    "count = 0\n",
    "for y_val, yh in zip(y, yhat):\n",
    "    count += 1 \n",
    "    if yhat[0] == yh:\n",
    "        correct += 1\n",
    "print(correct, count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff012f-1b25-4367-9b55-0fc5fb67d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model.\n",
      "\tNew loss: 0.040854188505200915 \n",
      "\tOld loss: 10000000000000000000000\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.04077000017426615 \n",
      "\tOld loss: 0.040854188505200915\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.040614937408480094 \n",
      "\tOld loss: 0.04077000017426615\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.04024686834843876 \n",
      "\tOld loss: 0.040614937408480094\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.04008646703168991 \n",
      "\tOld loss: 0.04024686834843876\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1000, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.04006213120055204 \n",
      "\tOld loss: 0.04008646703168991\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.03962295371875881 \n",
      "\tOld loss: 0.04006213120055204\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1000, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.03935210460898858 \n",
      "\tOld loss: 0.03962295371875881\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.038377427134784876 \n",
      "\tOld loss: 0.03935210460898858\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1000, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.03811433889848801 \n",
      "\tOld loss: 0.038377427134784876\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.034447676517771364 \n",
      "\tOld loss: 0.03811433889848801\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.029985954500096515 \n",
      "\tOld loss: 0.034447676517771364\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1000, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "New best model.\n",
      "\tNew loss: 0.028868374968960622 \n",
      "\tOld loss: 0.029985954500096515\n",
      "{'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.001, 'loss': 'absolute_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 500, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "def try_model(x,y, lr, estimators, max_depth, loss=\"squared_error\"):\n",
    "    model = GradientBoostingRegressor(loss=loss,learning_rate=lr, n_estimators=estimators, max_depth=max_depth)\n",
    "    \n",
    "    model.fit(x,y)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_score(model,x,y):\n",
    "    yhat = model.predict(x)\n",
    "    return mean_absolute_error(y,yhat)\n",
    "\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.002, 0.02, 0.2, 0.005, 0.05, 0.5]\n",
    "max_depth = [1, 3, 5, 10, None]\n",
    "estimators = [50, 100, 200, 500, 1000]\n",
    "\n",
    "\n",
    "best_m = None\n",
    "best_loss = 10000000000000000000000\n",
    "for lr in learning_rate:\n",
    "    for depth in max_depth:\n",
    "        for est in estimators:\n",
    "            m = try_model(X,y, lr, est, depth, loss=\"absolute_error\")\n",
    "            loss = model_score(m,X,y)\n",
    "            if loss < best_loss:\n",
    "                print(\"New best model.\\n\\tNew loss:\", loss, \"\\n\\tOld loss:\", best_loss)\n",
    "                best_loss = loss\n",
    "                best_m = m\n",
    "                print(m.get_params())\n",
    "            #else:\n",
    "                #print(\"New model not better.\\n\\tNew loss:\", loss, \"\\n\\tOld loss:\", best_loss)\n",
    "\n",
    "yhat = best_m.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2fe2f-e861-49a0-829d-341a6e4b12f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_model(y,yhat):\n",
    "   \n",
    "    fig, ax = plt.subplots(2)\n",
    "    \n",
    "    ax[0].plot(range(1,51), yhat[:50])\n",
    "    ax[0].plot( range(1,51), y[:50])\n",
    "\n",
    "    ax[1].plot(range(2000,2050), yhat[2000:2050])\n",
    "    \n",
    "    ax[1].plot(range(2000,2050), y[2000:2050] )\n",
    "    fig.suptitle(\"Result when feeding correct values as input\")\n",
    "    fig.savefig(\"teacher_forcing.png\")\n",
    "\n",
    "\n",
    "\n",
    "eval_model(y,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b76e02b-830b-428f-be1e-acb3816ac9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
