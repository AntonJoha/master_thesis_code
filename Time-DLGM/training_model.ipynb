{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5651277a-4947-47fd-9584-bc2d650f3849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n",
      "1.12.0+cu116\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from parse_data import get_data, get_modified_values, get_binary_values, make_data_scalar\n",
    "import numpy as np\n",
    "import random\n",
    "from data_gen import Datagen\n",
    "from recognition import Recognition\n",
    "from generator import Generator\n",
    "from evaluation import evaluate_model, bin_plot\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f4c9a1-56ec-4e52-abf2-e0fd1eb372dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Datagen(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871731c4-a079-4f95-a217-6583583456b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "# Hyperparameters\n",
    "sequence_length = [2*i for i in range(4,16)] # 2-20 increments of two\n",
    "hidden_layers = [1,2]*10 # 1 and 2\n",
    "hidden_1 = [2**i for i in range(5,10)] # 2^4 to 2^9\n",
    "hidden_2 =[2**i for i in range(5,10)] # 2^2 to 2^5\n",
    "variance = [0.001, 0.01, 0.005, 0.05]\n",
    "lr = [0.001, 0.01, 0.1, 0.005] # stop at 0.005\n",
    "data_probability = [i/5 for i in range(1,6)]\n",
    "noise_in_model = [True, False]\n",
    "epochs = 100\n",
    "optimizer = [optim.Adam, optim.SGD]\n",
    "print(hidden_layers)\n",
    "\n",
    "options = []\n",
    "\n",
    "for seq_len in sequence_length:\n",
    "    for layers in hidden_layers:\n",
    "        for h1 in hidden_1:\n",
    "            for h2 in hidden_2:\n",
    "                for l in lr:\n",
    "                    for v in variance:\n",
    "                        for p in data_probability:\n",
    "                            for n in noise_in_model:\n",
    "                                entry = {}\n",
    "                                entry[\"seq_len\"] = seq_len\n",
    "                                entry[\"layers\"] = layers\n",
    "                                entry[\"latent\"] = h1\n",
    "                                entry[\"hidden\"] = h2\n",
    "                                entry[\"l\"] = l\n",
    "                                entry[\"variance\"] = v\n",
    "                                entry[\"data_prob\"] = p\n",
    "                                entry[\"noise_model\"] = n\n",
    "                                options.append(entry)\n",
    "                \n",
    "                                         \n",
    "random.shuffle(options)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad534be-bb82-41da-87ff-929f023a02af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0.3088997579088398, 94.46990215778351]\n",
      "[10, 0.19448667219261717, 59.407810270786285]\n",
      "[20, 0.19143690353904674, 58.58940780162811]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m     l \u001b[38;5;241m=\u001b[39m loss(x, b, rec[\u001b[38;5;241m0\u001b[39m], rec[\u001b[38;5;241m1\u001b[39m], device, entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 67\u001b[0m     \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import torch.utils.data as data\n",
    "from itertools import chain\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "def loss(x, x_hat, mean, R, device=None, seq_len=1):\n",
    "\n",
    "\n",
    "    bce = nn.MSELoss( reduction='sum').to(device)\n",
    "    l = bce(x, x_hat)\n",
    "    amount = len(mean)\n",
    "    for m, r in zip(mean, R):\n",
    "        C = r @ r.transpose(-2,-1)\n",
    "        l += 0.5 * torch.sum(m.pow(2).sum(-1) \n",
    "                             + C.diagonal(dim1=-2,dim2=-1).sum(-1)\n",
    "                            - 2*C.diagonal(dim1=-2,dim2=-1).log().sum(-1) -1)\n",
    "        a = m.pow(2).sum(-1) + C.diagonal(dim1=-2,dim2=-1).sum(-1) - 2*C.diagonal(dim1=-2,dim2=-1).log().sum(-1)\n",
    "        \n",
    "    return l    \n",
    "\n",
    "best_model = None\n",
    "best_score = 10000000000000000\n",
    "batch_size = 50\n",
    "best_history= [0,0,0,0,0,0]\n",
    "for entry in options:\n",
    "    \n",
    "    x_d, y_d = gen.get_generated_data(entry[\"seq_len\"], entry[\"variance\"], entry[\"data_prob\"])\n",
    "    x_t, y_t = gen.get_true_data(entry[\"seq_len\"])\n",
    "    x_val, y_val = gen.get_test_data(entry[\"seq_len\"])\n",
    "\n",
    "\n",
    "    model_g = Generator(hidden_size=entry[\"hidden\"],\n",
    "                        latent_dim=entry[\"latent\"],\n",
    "                        output_dim=y_d[0].size()[0],\n",
    "                        layers=entry[\"layers\"],\n",
    "                        seq_len=batch_size,\n",
    "                        device=device)\n",
    "    model_r = Recognition(input_dim=x_d[0].size()[1],\n",
    "                          latent_dim=entry[\"latent\"],\n",
    "                          layers=entry[\"layers\"],\n",
    "                          device=device)\n",
    "\n",
    "    loader = data.DataLoader(data.TensorDataset(x_d, y_d), batch_size=batch_size, shuffle=True)\n",
    "    optimizer = optim.Adam(chain(model_r.parameters(), model_g.parameters()), lr=0.01)\n",
    "    #optimizer = optim.Adam(model_r.parameters())\n",
    "    history = []\n",
    "    bce = nn.BCELoss().to(device)\n",
    "    for e in range(epochs):\n",
    "        model_g.train()\n",
    "        model_r.train()\n",
    "\n",
    "\n",
    "        for x, y in loader:\n",
    "\n",
    "            x.to(device)\n",
    "            y.to(device)\n",
    "            if x.size()[0] < batch_size:\n",
    "                continue\n",
    "            if random.random() < 0.5:\n",
    "                continue\n",
    "            model_g.make_internal_state(batch_size)\n",
    "            rec = model_r(x)\n",
    "            model_g.set_xi(rec[-1])\n",
    "            b = model_g()\n",
    "            l = loss(x, b, rec[0], rec[1], device, entry[\"seq_len\"])\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "                        \n",
    "\n",
    "        \n",
    "        if e % 10 != 0:\n",
    "            continue\n",
    "        \n",
    "        count = 0\n",
    "        sum_loss = [0, 0]\n",
    "        for j in range(2):\n",
    "            for x, y in loader:\n",
    "                if x.size()[0] < batch_size:\n",
    "                    continue\n",
    "                model_g.eval()\n",
    "                model_g.make_internal_state(batch_size)\n",
    "                model_g.make_xi()\n",
    "                with torch.no_grad():\n",
    "                    model_g.make_internal_state(batch_size)\n",
    "                    rec = model_r(x)\n",
    "                    model_g.set_xi(rec[-1])\n",
    "                    b = model_g()\n",
    "                    l = loss(x, b, rec[0], rec[1], device, entry[\"seq_len\"])\n",
    "                    res = []\n",
    "                    \n",
    "                    l = bce(b, x).cpu()\n",
    "                    sum_loss[j] += l.item()\n",
    "                    count += 1\n",
    "                    \n",
    "        \n",
    "        \n",
    "        sum_loss[0] /= count\n",
    "     \n",
    "        \n",
    "        history.append([e, sum_loss[0], sum_loss[1]])\n",
    "        print(history[-1])\n",
    "\n",
    "        if len(history) > 15:\n",
    "            #if no real improvements are being done stop the training. \n",
    "            # but keep doing the training if the results without correctly feeding values get better\n",
    "            if abs(history[-15][1] - history[-1][1]) < 0.0001:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    if history[-1][1] < best_score:\n",
    "        print(\"New best model:\\nNew loss: \", history[-1], \"\\nOld loss:\", best_history[-1], \"\\nHistory:\" , history[-10:])\n",
    "        best_model = model_g\n",
    "        best_history = history\n",
    "        best_score = history[-1][1]\n",
    "        best_config = entry\n",
    "        evaluate_model(best_model, x_t, y_t,x_val,y_val, copy.deepcopy(entry))\n",
    "    else:\n",
    "        evaluate_model(model_g, x_t, y_t,x_val,y_val, copy.deepcopy(entry))\n",
    "        print(\"Old model still stands:\\nCurrent loss: \", history[-1], \"\\nBest loss:\", best_history[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6fd679-0e46-4e3d-b548-b02dfed77de9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.zeros(1,2,3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
