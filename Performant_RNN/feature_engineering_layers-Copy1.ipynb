{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecc6862-1f4c-4dbc-878a-28c97a990926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from parse_data import get_data, get_modified_values, get_binary_values, make_data_scalar\n",
    "from rnn_layers import PredictTime\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a932cf-5ecf-4c1f-9513-2400c8cb8686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]], device='cuda:0'),\n",
       " tensor([[1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.]], device='cuda:0'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_data(df, device):\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    prev = None\n",
    "\n",
    "    for row in df:\n",
    "        if prev is None:\n",
    "            prev = row\n",
    "        x_train.append(prev)\n",
    "        y_train.append(row)\n",
    "        prev = row\n",
    "    print(x_train[0].size())\n",
    "    return torch.stack(x_train).float().to(device),torch.stack(y_train).float().to(device)\n",
    "\n",
    "make_data(get_binary_values(get_data()), device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f71633-04f1-488e-8421-3df5644a11a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  1000\n",
      "torch.Size([8152, 1]) torch.Size([8152, 1])\n",
      "Epoch 0 Loss 34.4902\n",
      "Epoch 10 Loss 29.6071\n",
      "Epoch 20 Loss 29.6073\n",
      "Epoch 30 Loss 29.7210\n",
      "Epoch 40 Loss 29.5856\n",
      "Epoch 50 Loss 29.5787\n",
      "Epoch 60 Loss 29.5442\n",
      "Epoch 70 Loss 29.4789\n",
      "Epoch 80 Loss 29.2787\n",
      "Epoch 90 Loss 29.0951\n",
      "Epoch 100 Loss 29.0045\n",
      "Epoch 110 Loss 28.9091\n",
      "Epoch 120 Loss 28.7659\n",
      "Epoch 130 Loss 28.5753\n",
      "Epoch 140 Loss 28.4047\n",
      "Epoch 150 Loss 28.3873\n",
      "Epoch 160 Loss 28.3736\n",
      "Epoch 170 Loss 28.3641\n",
      "Epoch 180 Loss 28.3601\n",
      "Epoch 190 Loss 28.3494\n",
      "Epoch 200 Loss 28.3497\n",
      "Epoch 210 Loss 28.4289\n",
      "Epoch 220 Loss 28.3490\n",
      "Epoch 230 Loss 28.3480\n",
      "Epoch 240 Loss 29.8986\n",
      "Epoch 250 Loss 28.7615\n",
      "Epoch 260 Loss 28.5663\n",
      "Epoch 270 Loss 28.4569\n",
      "Epoch 280 Loss 28.4299\n",
      "Epoch 290 Loss 28.3863\n",
      "Epoch 300 Loss 28.3715\n",
      "Epoch 310 Loss 28.3596\n",
      "Epoch 320 Loss 28.3499\n",
      "Epoch 330 Loss 28.3411\n",
      "Epoch 340 Loss 28.3332\n",
      "Epoch 350 Loss 28.3353\n",
      "Epoch 360 Loss 28.4538\n",
      "Epoch 370 Loss 28.3710\n",
      "Epoch 380 Loss 28.3634\n",
      "Epoch 390 Loss 28.3245\n",
      "Epoch 400 Loss 28.3922\n",
      "Epoch 410 Loss 28.5065\n",
      "Epoch 420 Loss 28.3573\n",
      "Epoch 430 Loss 28.3338\n",
      "Epoch 440 Loss 28.5211\n",
      "Epoch 450 Loss 28.5609\n",
      "Epoch 460 Loss 28.4199\n",
      "Epoch 470 Loss 28.4074\n",
      "Epoch 480 Loss 28.3501\n",
      "Epoch 490 Loss 28.3329\n",
      "Epoch 500 Loss 28.3214\n",
      "Epoch 510 Loss 28.3079\n",
      "Epoch 520 Loss 28.3759\n",
      "Epoch 530 Loss 28.4148\n",
      "Epoch 540 Loss 29.5840\n",
      "Epoch 550 Loss 28.3733\n",
      "Epoch 560 Loss 28.6468\n",
      "Epoch 570 Loss 28.3496\n",
      "Epoch 580 Loss 28.2996\n",
      "Epoch 590 Loss 28.3125\n",
      "Epoch 600 Loss 28.4245\n",
      "Epoch 610 Loss 28.3265\n",
      "Epoch 620 Loss 28.4192\n",
      "Epoch 630 Loss 28.2908\n",
      "Epoch 640 Loss 28.3113\n",
      "Epoch 650 Loss 28.4478\n",
      "Epoch 660 Loss 28.2832\n",
      "Epoch 670 Loss 28.3035\n",
      "Epoch 680 Loss 28.5625\n",
      "Epoch 690 Loss 28.3544\n",
      "Epoch 700 Loss 28.2853\n",
      "Epoch 710 Loss 28.2896\n",
      "Epoch 720 Loss 28.2724\n",
      "Epoch 730 Loss 28.2844\n",
      "Epoch 740 Loss 28.2503\n",
      "Epoch 750 Loss 28.2426\n",
      "Epoch 760 Loss 28.3236\n",
      "Epoch 770 Loss 28.2842\n",
      "Epoch 780 Loss 28.2356\n",
      "Epoch 790 Loss 28.2221\n",
      "Epoch 800 Loss 28.2924\n",
      "Epoch 810 Loss 28.5728\n",
      "Epoch 820 Loss 28.2560\n",
      "Epoch 830 Loss 28.2055\n",
      "Epoch 840 Loss 28.3041\n",
      "Epoch 850 Loss 28.2441\n",
      "Epoch 860 Loss 29.5793\n",
      "Epoch 870 Loss 28.1714\n",
      "Epoch 880 Loss 28.1748\n",
      "Epoch 890 Loss 28.1396\n",
      "Epoch 900 Loss 28.1188\n",
      "Epoch 910 Loss 28.1007\n",
      "Epoch 920 Loss 28.0691\n",
      "Epoch 930 Loss 28.0454\n",
      "Epoch 940 Loss 28.0094\n",
      "Epoch 950 Loss 27.9752\n",
      "Epoch 960 Loss 27.9292\n",
      "Epoch 970 Loss 30.1149\n",
      "Epoch 980 Loss 29.6842\n",
      "Epoch 990 Loss 28.2034\n",
      "Epoch 1000 Loss 28.1248\n",
      "Epoch 1010 Loss 28.0988\n",
      "Epoch 1020 Loss 28.0003\n",
      "Epoch 1030 Loss 27.9328\n",
      "Epoch 1040 Loss 28.0032\n",
      "Epoch 1050 Loss 27.9482\n",
      "Epoch 1060 Loss 27.8292\n",
      "Epoch 1070 Loss 29.5853\n",
      "Epoch 1080 Loss 27.6976\n",
      "Epoch 1090 Loss 27.7132\n",
      "Epoch 1100 Loss 29.6358\n",
      "Epoch 1110 Loss 27.4912\n",
      "Epoch 1120 Loss 27.3568\n",
      "Epoch 1130 Loss 27.3727\n",
      "Epoch 1140 Loss 27.2572\n",
      "Epoch 1150 Loss 27.1217\n",
      "Epoch 1160 Loss 26.9655\n",
      "Epoch 1170 Loss 26.7988\n",
      "Epoch 1180 Loss 26.6373\n",
      "Epoch 1190 Loss 26.6619\n",
      "Epoch 1200 Loss 26.4592\n",
      "Epoch 1210 Loss 26.2297\n",
      "Epoch 1220 Loss 26.1290\n",
      "Epoch 1230 Loss 26.0034\n",
      "Epoch 1240 Loss 25.8351\n",
      "Epoch 1250 Loss 25.6755\n",
      "Epoch 1260 Loss 25.6064\n",
      "Epoch 1270 Loss 25.3341\n",
      "Epoch 1280 Loss 26.4593\n",
      "Epoch 1290 Loss 25.9669\n",
      "Epoch 1300 Loss 25.6425\n",
      "Epoch 1310 Loss 25.5498\n",
      "Epoch 1320 Loss 25.1903\n",
      "Epoch 1330 Loss 24.9578\n",
      "Epoch 1340 Loss 29.5405\n",
      "Epoch 1350 Loss 24.6143\n",
      "Epoch 1360 Loss 24.4680\n",
      "Epoch 1370 Loss 24.2937\n",
      "Epoch 1380 Loss 24.1823\n",
      "Epoch 1390 Loss 23.8952\n",
      "Epoch 1400 Loss 26.1166\n",
      "Epoch 1410 Loss 28.4615\n",
      "Epoch 1420 Loss 26.7942\n",
      "Epoch 1430 Loss 25.9215\n",
      "Epoch 1440 Loss 25.3357\n",
      "Epoch 1450 Loss 24.7726\n",
      "Epoch 1460 Loss 24.3370\n",
      "Epoch 1470 Loss 24.0030\n",
      "Epoch 1480 Loss 23.7663\n",
      "Epoch 1490 Loss 23.5431\n",
      "Epoch 1500 Loss 23.3030\n",
      "Epoch 1510 Loss 29.7127\n",
      "Epoch 1520 Loss 31.0178\n",
      "Epoch 1530 Loss 29.7510\n",
      "Epoch 1540 Loss 29.7363\n",
      "Epoch 1550 Loss 29.3749\n",
      "Epoch 1560 Loss 29.1783\n",
      "Epoch 1570 Loss 29.1331\n",
      "Epoch 1580 Loss 29.0241\n",
      "Epoch 1590 Loss 29.7241\n",
      "Epoch 1600 Loss 28.9103\n",
      "Epoch 1610 Loss 29.6562\n",
      "Epoch 1620 Loss 29.7262\n",
      "Epoch 1630 Loss 28.7181\n",
      "Epoch 1640 Loss 28.7028\n",
      "Epoch 1650 Loss 28.5954\n",
      "Epoch 1660 Loss 28.6244\n",
      "Epoch 1670 Loss 29.6229\n",
      "Epoch 1680 Loss 28.5083\n",
      "Epoch 1690 Loss 28.4797\n",
      "Epoch 1700 Loss 28.4689\n",
      "Epoch 1710 Loss 28.4586\n",
      "Epoch 1720 Loss 28.4240\n",
      "Epoch 1730 Loss 29.5665\n",
      "Epoch 1740 Loss 28.4567\n",
      "Epoch 1750 Loss 28.4348\n",
      "Epoch 1760 Loss 28.4116\n",
      "Epoch 1770 Loss 28.3999\n",
      "Epoch 1780 Loss 28.4055\n",
      "Epoch 1790 Loss 29.6035\n",
      "Epoch 1800 Loss 28.4068\n",
      "Epoch 1810 Loss 28.4004\n",
      "Epoch 1820 Loss 29.5861\n",
      "Epoch 1830 Loss 28.3927\n",
      "Epoch 1840 Loss 28.4349\n",
      "Epoch 1850 Loss 28.3717\n",
      "Epoch 1860 Loss 28.3726\n",
      "Epoch 1870 Loss 28.4008\n",
      "Epoch 1880 Loss 28.3848\n",
      "Epoch 1890 Loss 28.3816\n",
      "Epoch 1900 Loss 28.3627\n",
      "Epoch 1910 Loss 28.3571\n",
      "Epoch 1920 Loss 28.3796\n",
      "Epoch 1930 Loss 28.4471\n",
      "Epoch 1940 Loss 28.4514\n",
      "Epoch 1950 Loss 28.5022\n",
      "Epoch 1960 Loss 28.4407\n",
      "Epoch 1970 Loss 28.3977\n",
      "Epoch 1980 Loss 28.3798\n",
      "Epoch 1990 Loss 28.3796\n",
      "Epoch 2000 Loss 28.3867\n",
      "Epoch 2010 Loss 28.4027\n",
      "Epoch 2020 Loss 28.3711\n",
      "Epoch 2030 Loss 28.3605\n",
      "Epoch 2040 Loss 28.3571\n",
      "Epoch 2050 Loss 28.3504\n",
      "Epoch 2060 Loss 28.3940\n",
      "Epoch 2070 Loss 28.3475\n",
      "Epoch 2080 Loss 28.3444\n",
      "Epoch 2090 Loss 29.7266\n",
      "Epoch 2100 Loss 28.3427\n",
      "Epoch 2110 Loss 28.3375\n",
      "Epoch 2120 Loss 28.3361\n",
      "Epoch 2130 Loss 28.3329\n",
      "Epoch 2140 Loss 28.3307\n",
      "Epoch 2150 Loss 28.3288\n",
      "Epoch 2160 Loss 29.7012\n",
      "Epoch 2170 Loss 28.3274\n",
      "Epoch 2180 Loss 28.3236\n",
      "Epoch 2190 Loss 28.3243\n",
      "Epoch 2200 Loss 28.3203\n",
      "Epoch 2210 Loss 29.6484\n",
      "Epoch 2220 Loss 29.5806\n",
      "Epoch 2230 Loss 28.3153\n",
      "Epoch 2240 Loss 28.3215\n",
      "Epoch 2250 Loss 28.3146\n",
      "Epoch 2260 Loss 28.3260\n",
      "Epoch 2270 Loss 29.6982\n",
      "Epoch 2280 Loss 28.3142\n",
      "Epoch 2290 Loss 29.7584\n",
      "Epoch 2300 Loss 28.5779\n",
      "Epoch 2310 Loss 28.3456\n",
      "Epoch 2320 Loss 28.3170\n",
      "Epoch 2330 Loss 28.3275\n",
      "Epoch 2340 Loss 28.3147\n",
      "Epoch 2350 Loss 28.3064\n",
      "Epoch 2360 Loss 28.3400\n",
      "Epoch 2370 Loss 28.3182\n",
      "Epoch 2380 Loss 29.6523\n",
      "Epoch 2390 Loss 28.8281\n",
      "Epoch 2400 Loss 29.7383\n",
      "Epoch 2410 Loss 28.4066\n",
      "Epoch 2420 Loss 28.3627\n",
      "Epoch 2430 Loss 28.3356\n",
      "Epoch 2440 Loss 28.3355\n",
      "Epoch 2450 Loss 28.3143\n",
      "Epoch 2460 Loss 28.3109\n",
      "Epoch 2470 Loss 28.3121\n",
      "Epoch 2480 Loss 28.3028\n",
      "Epoch 2490 Loss 28.3034\n",
      "Epoch 2500 Loss 28.2979\n",
      "Epoch 2510 Loss 28.3059\n",
      "Epoch 2520 Loss 29.8296\n",
      "Epoch 2530 Loss 28.2963\n",
      "Epoch 2540 Loss 28.2896\n",
      "Epoch 2550 Loss 28.2935\n",
      "Epoch 2560 Loss 29.9180\n",
      "Epoch 2570 Loss 28.3116\n",
      "Epoch 2580 Loss 28.2991\n",
      "Epoch 2590 Loss 28.3095\n",
      "Epoch 2600 Loss 28.2917\n",
      "Epoch 2610 Loss 29.6919\n",
      "Epoch 2620 Loss 28.2918\n",
      "Epoch 2630 Loss 28.2835\n",
      "Epoch 2640 Loss 28.2845\n",
      "Epoch 2650 Loss 29.7342\n",
      "Epoch 2660 Loss 28.2866\n",
      "Epoch 2670 Loss 28.2839\n",
      "Epoch 2680 Loss 28.2783\n",
      "Epoch 2690 Loss 28.2730\n",
      "Epoch 2700 Loss 28.2813\n",
      "Epoch 2710 Loss 28.2754\n",
      "Epoch 2720 Loss 28.2693\n",
      "Epoch 2730 Loss 29.7673\n",
      "Epoch 2740 Loss 28.2688\n",
      "Epoch 2750 Loss 28.2782\n",
      "Epoch 2760 Loss 29.6676\n",
      "Epoch 2770 Loss 29.7605\n",
      "Epoch 2780 Loss 29.7013\n",
      "Epoch 2790 Loss 28.2839\n",
      "Epoch 2800 Loss 28.2643\n",
      "Epoch 2810 Loss 28.2622\n",
      "Epoch 2820 Loss 28.2621\n",
      "Epoch 2830 Loss 28.4372\n",
      "Epoch 2840 Loss 28.7258\n",
      "Epoch 2850 Loss 28.4409\n",
      "Epoch 2860 Loss 29.9506\n",
      "Epoch 2870 Loss 28.3216\n",
      "Epoch 2880 Loss 29.8686\n",
      "Epoch 2890 Loss 28.2953\n",
      "Epoch 2900 Loss 29.7072\n",
      "Epoch 2910 Loss 29.9132\n",
      "Epoch 2920 Loss 28.2839\n",
      "Epoch 2930 Loss 28.2765\n",
      "Epoch 2940 Loss 28.2789\n",
      "Epoch 2950 Loss 28.2833\n",
      "Epoch 2960 Loss 29.8229\n",
      "Epoch 2970 Loss 28.2606\n",
      "Epoch 2980 Loss 28.2772\n",
      "Epoch 2990 Loss 29.8366\n",
      "Epoch 3000 Loss 29.9439\n",
      "Epoch 3010 Loss 29.7908\n",
      "Epoch 3020 Loss 28.2474\n",
      "Epoch 3030 Loss 28.2677\n",
      "Epoch 3040 Loss 28.2602\n",
      "Epoch 3050 Loss 28.3087\n",
      "Epoch 3060 Loss 29.9348\n",
      "Epoch 3070 Loss 29.7718\n",
      "Epoch 3080 Loss 28.2517\n",
      "Epoch 3090 Loss 28.2513\n",
      "Epoch 3100 Loss 28.2594\n",
      "Epoch 3110 Loss 28.2446\n",
      "Epoch 3120 Loss 28.2585\n",
      "Epoch 3130 Loss 29.8963\n",
      "Epoch 3140 Loss 28.2407\n",
      "Epoch 3150 Loss 29.8819\n",
      "Epoch 3160 Loss 28.2412\n",
      "Epoch 3170 Loss 28.2433\n",
      "Epoch 3180 Loss 28.2307\n",
      "Epoch 3190 Loss 28.2548\n",
      "Epoch 3200 Loss 29.6491\n",
      "Epoch 3210 Loss 29.6861\n",
      "Epoch 3220 Loss 29.7183\n",
      "Epoch 3230 Loss 29.8063\n",
      "Epoch 3240 Loss 29.8301\n",
      "Epoch 3250 Loss 29.8462\n",
      "Epoch 3260 Loss 29.8133\n",
      "Epoch 3270 Loss 28.2352\n",
      "Epoch 3280 Loss 29.8879\n",
      "Epoch 3290 Loss 28.2636\n",
      "Epoch 3300 Loss 29.7113\n",
      "Epoch 3310 Loss 28.2231\n",
      "Epoch 3320 Loss 28.2409\n",
      "Epoch 3330 Loss 29.5527\n",
      "Epoch 3340 Loss 28.2372\n",
      "Epoch 3350 Loss 28.2333\n",
      "Epoch 3360 Loss 28.2092\n",
      "Epoch 3370 Loss 28.2140\n",
      "Epoch 3380 Loss 28.2217\n",
      "Epoch 3390 Loss 28.2188\n",
      "Epoch 3400 Loss 30.1332\n",
      "Epoch 3410 Loss 28.2054\n",
      "Epoch 3420 Loss 28.2072\n",
      "Epoch 3430 Loss 30.0610\n",
      "Epoch 3440 Loss 30.0534\n",
      "Epoch 3450 Loss 28.2197\n",
      "Epoch 3460 Loss 28.2221\n",
      "Epoch 3470 Loss 28.1980\n",
      "Epoch 3480 Loss 28.2153\n",
      "Epoch 3490 Loss 29.7399\n",
      "Epoch 3500 Loss 30.1230\n",
      "Epoch 3510 Loss 28.2348\n",
      "Epoch 3520 Loss 28.2051\n",
      "Epoch 3530 Loss 28.2041\n",
      "Epoch 3540 Loss 29.5840\n",
      "Epoch 3550 Loss 28.1754\n",
      "Epoch 3560 Loss 29.9422\n",
      "Epoch 3570 Loss 30.2563\n",
      "Epoch 3580 Loss 28.1735\n",
      "Epoch 3590 Loss 28.1724\n",
      "Epoch 3600 Loss 28.2284\n",
      "Epoch 3610 Loss 28.1742\n",
      "Epoch 3620 Loss 28.2043\n",
      "Epoch 3630 Loss 29.6384\n",
      "Epoch 3640 Loss 29.5796\n",
      "Epoch 3650 Loss 28.1582\n",
      "Epoch 3660 Loss 29.9497\n",
      "Epoch 3670 Loss 28.1506\n",
      "Epoch 3680 Loss 29.6968\n",
      "Epoch 3690 Loss 28.1472\n",
      "Epoch 3700 Loss 28.1651\n",
      "Epoch 3710 Loss 29.8176\n",
      "Epoch 3720 Loss 30.1430\n",
      "Epoch 3730 Loss 28.1399\n",
      "Epoch 3740 Loss 28.1580\n",
      "Epoch 3750 Loss 28.1364\n",
      "Epoch 3760 Loss 29.7375\n",
      "Epoch 3770 Loss 28.1287\n",
      "Epoch 3780 Loss 29.8186\n",
      "Epoch 3790 Loss 28.1167\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "batch_size = 100\n",
    "x_d, y_d = make_data_scalar(get_data(), device)\n",
    "\n",
    "print(y_d.size(), x_d.size())\n",
    "\n",
    "model = PredictTime(input_size=x_d[0].size()[0],\n",
    "                    output_size=y_d[0].size()[0],\n",
    "                    hidden_layers=3,\n",
    "                    hidden_size=50, device=device).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "loader = data.DataLoader(data.TensorDataset(x_d,y_d), batch_size=batch_size)\n",
    "epochs = 5000\n",
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    #print(next(iter(loader)))\n",
    "    model.clean_state()\n",
    "    res = []\n",
    "    \n",
    "    \n",
    "    if random.random() < e/(epochs*2):\n",
    "        model.teacher_forcing = False\n",
    "    else:\n",
    "        model.teacher_forcing = True\n",
    "    \n",
    "    for x, y in loader:\n",
    "        \n",
    "        if random.random() < -1:\n",
    "            continue\n",
    "        model.init_state()\n",
    "        \n",
    "        \n",
    "        y_pred = model(x)\n",
    "        l = loss(y_pred, y)\n",
    "        res.append(l)\n",
    "        #print(y_pred, y)\n",
    "    \n",
    "    \n",
    "    l = res[0]\n",
    "    for i in res[1:]:\n",
    "        l += i\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    if e % 10 != 0:\n",
    "        continue\n",
    "    #clear_output(wait=True)\n",
    "    sum_loss = 0\n",
    "    #print(list(model.parameters())[-1])\n",
    "\n",
    "    for x, y in loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            sum_loss += np.sqrt(loss(y_pred, y).cpu())\n",
    "    \n",
    "    print(\"Epoch %d Loss %.4f\" % (e, sum_loss))\n",
    "    \n",
    "    \n",
    "#for d in df.values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e66f0-9596-4127-b7c4-835709e375e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_model(x,y,m):\n",
    "    df = get_data() \n",
    "    maxtime = df.max()[1]\n",
    "\n",
    "    res = []\n",
    "    m.eval()\n",
    "    m.clean_state()\n",
    "    count  = 1\n",
    "    prev = x[0][0]\n",
    "    for i in x:\n",
    "        t = torch.tensor([[prev]]).to(device)\n",
    "        prev = i\n",
    "        val = m(t)\n",
    "        count += 1\n",
    "        res.append(val.detach().cpu()[0])\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    print(np.sqrt(loss(torch.tensor(res).to(device),y.squeeze()).cpu()))\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(2)\n",
    "    \n",
    "    ax[0].plot(range(1,51), res[:50])\n",
    "    ax[0].plot( range(1,51), y[:50].cpu())\n",
    "\n",
    "    ax[1].plot(range(2000,2050), res[2000:2050])\n",
    "    \n",
    "    ax[1].plot(range(2000,2050), y[2000:2050].cpu() )\n",
    "    fig.suptitle(\"Result when feeding correct values as input\")\n",
    "    fig.savefig(\"teacher_forcing.png\")\n",
    "\n",
    "\n",
    "amount = 1000\n",
    "eval_model(x_d,y_d,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa2663-d7c6-4407-8f5d-e8e3ab650e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def eval_model(x,y,m):\n",
    "    df = get_data() \n",
    "    maxtime = df.max()[1]\n",
    "\n",
    "    res = []\n",
    "    m.eval()\n",
    "    m.clean_state()\n",
    "    count  = 1\n",
    "    prev = x[0][0]\n",
    "    for i in x:\n",
    "        t = torch.tensor([[prev]]).to(device)\n",
    "        prev = i\n",
    "        val = m(t)\n",
    "        prev = val\n",
    "        count += 1\n",
    "        res.append(val.detach().cpu()[0])\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    print(np.sqrt(loss(torch.tensor(res).to(device),y.squeeze()).cpu()))\n",
    "    \n",
    "    fig, ax = plt.subplots(2)\n",
    "    \n",
    "    ax[0].plot(range(1,51), res[:50])\n",
    "    ax[0].plot( range(1,51), y[:50].cpu())\n",
    "\n",
    "    ax[1].plot(range(2000,2050), res[2000:2050])\n",
    "    \n",
    "    ax[1].plot(range(2000,2050), y[2000:2050].cpu() )\n",
    "    fig.suptitle(\"Result without feeding correct values as input\")\n",
    "    fig.savefig(\"teacher_forcing_not_correct.png\")\n",
    "    \n",
    "amount = 1000\n",
    "eval_model(x_d,y_d,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186992f6-b7a6-446e-9556-d36b32bb5bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
