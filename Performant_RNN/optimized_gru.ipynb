{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75639b7e-d5d2-4e27-ad5a-39f44e55f70d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4b1a05-7c0c-4eb8-847a-3e596ff4f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "from gru_parameter_tuning import train_model\n",
    "from rnn_layers import PredictTime\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from parse_data import get_data, get_modified_values, get_binary_values, make_data_scalar\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = [5,15,50,100,200]\n",
    "hidden_layers = [2, 5, 10]\n",
    "hidden_size = [10, 25, 50, 100]\n",
    "lr = [0.001, 0.01, 0.1, 0.005, 0.05,0.5]\n",
    "epochs = 5000\n",
    "optimizer = [optim.Adam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1db29a8a-1fb6-4430-b650-5010f29295ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0.46]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1.0, 0.46, 1.0]\n",
      "[0, 0, 0, 0, 0, 0, 1.0, 0.46, 1.0, 0.46]\n",
      "[0, 0, 0, 0, 0, 1.0, 0.46, 1.0, 0.46, 1.0]\n",
      "[0, 0, 0, 0, 1.0, 0.46, 1.0, 0.46, 1.0, 0.0]\n",
      "[0, 0, 0, 1.0, 0.46, 1.0, 0.46, 1.0, 0.0, 1.0]\n",
      "[0, 0, 1.0, 0.46, 1.0, 0.46, 1.0, 0.0, 1.0, 1.0]\n",
      "[0, 1.0, 0.46, 1.0, 0.46, 1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "tensor([[1.0000],\n",
      "        [0.4600],\n",
      "        [1.0000],\n",
      "        ...,\n",
      "        [0.4600],\n",
      "        [0.5900],\n",
      "        [0.5900]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def make_data(df, device, seq_len):\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    prev = []\n",
    "    m = df.max()[0]\n",
    "    for row in df.values:\n",
    "        \n",
    "        if len(prev) < seq_len:\n",
    "            before = [0]*(seq_len - len(prev))\n",
    "            for a in prev:\n",
    "                before.append(a)\n",
    "            print(before)\n",
    "            x_train.append(torch.tensor(before))\n",
    "        else:   \n",
    "            x_train.append(torch.tensor(prev[-seq_len:]))\n",
    "        y_train.append(row[0]/m)\n",
    "        prev.append(row[0]/m)\n",
    "    return torch.stack(x_train).to(device), torch.tensor(y_train).to(device).unsqueeze(dim=1)\n",
    "\n",
    "X, y = make_data(get_data(), None, 10)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d020990d-21c4-4f03-9e52-b62014e7ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def draw_model(m, x , y, conf):\n",
    "    df = get_data() \n",
    "\n",
    "    res = []\n",
    "    m.eval()\n",
    "    m.clean_state()\n",
    "    prev = x[0][0]\n",
    "    for i in x:\n",
    "        t = torch.tensor([[prev]]).to(device)\n",
    "        prev = i\n",
    "        val = m(t)\n",
    "        res.append(val.detach().cpu()[0])\n",
    "\n",
    "    fig, ax = plt.subplots(2)\n",
    "    \n",
    "    ax[0].plot(range(1,51), res[:50])\n",
    "    ax[0].plot( range(1,51), y[:50].cpu())\n",
    "\n",
    "    ax[1].plot(range(2000,2050), res[2000:2050])\n",
    "    \n",
    "    ax[1].plot(range(2000,2050), y[2000:2050].cpu() )\n",
    "    fig.suptitle(\"%s\" % conf)\n",
    "    fig.savefig(\"images/forcing%s.png\" % conf)\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    m.eval()\n",
    "    m.clean_state()\n",
    "    prev = x[0][0]\n",
    "    for i in x:\n",
    "        t = torch.tensor([[prev]]).to(device)\n",
    "        val = m(t)\n",
    "        prev = val\n",
    "        res.append(val.detach().cpu()[0])\n",
    "\n",
    "    fig, ax = plt.subplots(2)\n",
    "    \n",
    "    ax[0].plot(range(1,51), res[:50])\n",
    "    ax[0].plot( range(1,51), y[:50].cpu())\n",
    "\n",
    "    ax[1].plot(range(2000,2050), res[2000:2050])\n",
    "    \n",
    "    ax[1].plot(range(2000,2050), y[2000:2050].cpu() )\n",
    "    fig.suptitle(\"%s\" % conf)\n",
    "    fig.savefig(\"images/not_forcing%s.png\" % conf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ac1b1a-c4fb-4952-9716-6983d1b8595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "options = []\n",
    "\n",
    "for seq_len in sequence_length:\n",
    "    for layers in hidden_layers:\n",
    "        for hidden in hidden_size:\n",
    "            for forcing in [True, False]:\n",
    "                for l in lr:\n",
    "                    entry = {}\n",
    "                    entry[\"seq_len\"] = seq_len\n",
    "                    entry[\"layers\"] = layers\n",
    "                    entry[\"hidden\"] = hidden\n",
    "                    entry[\"forcing\"] = forcing\n",
    "                    entry[\"l\"] = l\n",
    "                    options.append(entry)\n",
    "                \n",
    "                                         \n",
    "random.shuffle(options)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f4de2-78fb-4d25-b509-f9e272a6995c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value:  1000\n",
      "Running model:  {'seq_len': 5, 'layers': 10, 'hidden': 100, 'forcing': False, 'l': 0.5}  epochs:  5000\n",
      "New best model:\n",
      "New loss:  [4995, tensor(0.3179), tensor(0.3409)] \n",
      "Old loss:  \n",
      "History: [[4860, tensor(0.3236), tensor(0.3435)], [4875, tensor(0.3575), tensor(0.3591)], [4890, tensor(0.3701), tensor(0.3779)], [4905, tensor(0.3196), tensor(0.3598)], [4920, tensor(0.3253), tensor(0.3610)], [4935, tensor(0.3370), tensor(0.3336)], [4950, tensor(0.3535), tensor(0.3531)], [4965, tensor(0.3191), tensor(0.3418)], [4980, tensor(0.3667), tensor(0.3672)], [4995, tensor(0.3179), tensor(0.3409)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ary = asanyarray(ary)\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model:  {'seq_len': 5, 'layers': 10, 'hidden': 25, 'forcing': True, 'l': 0.1}  epochs:  5000\n",
      "New best model:\n",
      "New loss:  [4995, tensor(0.3033), tensor(0.3033)] \n",
      "Old loss: [4995, tensor(0.3179), tensor(0.3409)] \n",
      "History: [[4860, tensor(0.3037), tensor(0.3037)], [4875, tensor(0.3040), tensor(0.3040)], [4890, tensor(0.3046), tensor(0.3046)], [4905, tensor(0.3037), tensor(0.3037)], [4920, tensor(0.3032), tensor(0.3032)], [4935, tensor(0.3038), tensor(0.3038)], [4950, tensor(0.3037), tensor(0.3037)], [4965, tensor(0.3036), tensor(0.3036)], [4980, tensor(0.3036), tensor(0.3036)], [4995, tensor(0.3033), tensor(0.3033)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ary = asanyarray(ary)\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model:  {'seq_len': 50, 'layers': 2, 'hidden': 100, 'forcing': False, 'l': 0.001}  epochs:  5000\n",
      "New best model:\n",
      "New loss:  [4995, tensor(0.2715), tensor(0.3119)] \n",
      "Old loss: [4995, tensor(0.3033), tensor(0.3033)] \n",
      "History: [[4860, tensor(0.3111), tensor(0.3110)], [4875, tensor(0.2763), tensor(0.3114)], [4890, tensor(0.2786), tensor(0.3107)], [4905, tensor(0.2824), tensor(0.3085)], [4920, tensor(0.2822), tensor(0.3124)], [4935, tensor(0.2757), tensor(0.3097)], [4950, tensor(0.2797), tensor(0.3096)], [4965, tensor(0.3089), tensor(0.3088)], [4980, tensor(0.2758), tensor(0.3090)], [4995, tensor(0.2715), tensor(0.3119)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:65: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  ary = asanyarray(ary)\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model:  {'seq_len': 50, 'layers': 2, 'hidden': 50, 'forcing': True, 'l': 0.05}  epochs:  5000\n",
      "Old model still stands:\n",
      "Current loss:  [4995, tensor(0.3051), tensor(0.3051)] \n",
      "Best loss: [4995, tensor(0.2715), tensor(0.3119)]\n",
      "Running model:  {'seq_len': 15, 'layers': 10, 'hidden': 25, 'forcing': False, 'l': 0.1}  epochs:  5000\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "x_d, y_d = make_data_scalar(get_data(), device)\n",
    "\n",
    "best_model = None\n",
    "best_history = [\"\"]\n",
    "best_score = 100000000000000\n",
    "for entry in options:\n",
    "    model = PredictTime(input_size=x_d[0].size()[0],\n",
    "                        output_size=y_d[0].size()[0],\n",
    "                        hidden_layers=entry[\"layers\"],\n",
    "                        hidden_size=entry[\"hidden\"],\n",
    "                        device=device).to(device)\n",
    "    print(\"Running model: \", entry, \" epochs: \", epochs)\n",
    "    #%lprun -f train_model train_model(model=model,x_data=x_d,y_data=y_d,sequence_length=entry[\"seq_len\"],epochs=epochs,loss=nn.BCELoss(),optimizer=optim.Adam,strict_teacher_forcing=entry[\"forcing\"])\n",
    "    #break\n",
    "    model, history = train_model(model=model,\n",
    "                                 x_data=x_d,\n",
    "                                 y_data=y_d,\n",
    "                                 sequence_length=entry[\"seq_len\"],\n",
    "                                 epochs=epochs,\n",
    "                                 loss=nn.BCELoss(),\n",
    "                                 optimizer=optim.Adam,\n",
    "                                 strict_teacher_forcing=entry[\"forcing\"])\n",
    "    if history[-1][1] < best_score:\n",
    "        print(\"New best model:\\nNew loss: \", history[-1], \"\\nOld loss:\", best_history[-1], \"\\nHistory:\" , history[-10:])\n",
    "        best_model = model\n",
    "        best_history = history\n",
    "        best_score = history[-1][1]\n",
    "        draw_model(best_model, x_d, y_d, str(entry).replace(\" \", \"\"))\n",
    "    else:\n",
    "        print(\"Old model still stands:\\nCurrent loss: \", history[-1], \"\\nBest loss:\", best_history[-1])\n",
    "                                 \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7150a44-2202-4ad6-b51d-e643d85c37bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
